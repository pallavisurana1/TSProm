{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fab0ca6-4aa5-49ed-a5c1-3d29972ceb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84fcd530-0f15-44ad-8b72-2777184acca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/july_2025_mmseq/jobs.json\", \"r\") as f:\n",
    "    jobs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3af14e0-88c4-44fb-b9a5-53ff3372663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "base_path = \"/data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/july_2025_mmseq\"\n",
    "base_model_nms = [\"TSp_vs_nonProm\", \"TSp_vs_genNullseqs\"]\n",
    "length_nm = \"3k\"\n",
    "# subsets = [\"tspAll_\", \"tspliver_\", \"tsptestis_\", \"tspbrain_\"]\n",
    "subsets = [\"tspmuscle_\"]\n",
    "\n",
    "data_path = \"/data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/data/jul_2025/split\"\n",
    "tokenizer_name = 'jaandoui/DNABERT2-AttentionExtracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e067f8-f474-433d-a4e8-c003e871ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Collected 2 jobs\n",
      "** TSp_vs_nonProm | tspmuscle_nonPromHu | lr3e-5_ep10 | checkpoint-600\n",
      "** TSp_vs_genNullseqs | tspmuscle_genNullseqs | lr3e-5_ep10 | checkpoint-300\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "\n",
    "for base_model_nm in base_model_nms:\n",
    "    length_dir = os.path.join(base_path, base_model_nm, length_nm)\n",
    "    if not os.path.exists(length_dir):\n",
    "        print(f\"‚ö†Ô∏è Missing length dir: {length_dir}\")\n",
    "        continue\n",
    "\n",
    "    # All dirs under length_dir\n",
    "    all_subdirs = [d for d in os.listdir(length_dir) if os.path.isdir(os.path.join(length_dir, d))]\n",
    "\n",
    "    # Match subset dirs by prefix\n",
    "    matched_subdirs = [d for d in all_subdirs if any(d.startswith(pref) for pref in subsets)]\n",
    "    if not matched_subdirs:\n",
    "        print(f\"‚ö†Ô∏è No matching subset dirs under {length_dir}\")\n",
    "        continue\n",
    "\n",
    "    for subset in matched_subdirs:\n",
    "        search_dir = os.path.join(length_dir, subset)\n",
    "\n",
    "        # lr dirs: accept anything starting with \"lr3e-5\"\n",
    "        lr_dirs = [d for d in os.listdir(search_dir) if d.startswith(\"lr3e-5\")]\n",
    "        if not lr_dirs:\n",
    "            print(f\"‚ö†Ô∏è No lr3e-5 dirs under {search_dir}\")\n",
    "            continue\n",
    "\n",
    "        for lr_dir in lr_dirs:\n",
    "            model_dir = os.path.join(search_dir, lr_dir)\n",
    "            checkpoints = [d for d in os.listdir(model_dir) if d.startswith(\"checkpoint-\")]\n",
    "            if not checkpoints:\n",
    "                print(f\"‚ö†Ô∏è No checkpoints in {model_dir}\")\n",
    "                continue\n",
    "\n",
    "            # Pick latest checkpoint\n",
    "            latest_ckpt = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "            model_path = os.path.join(model_dir, latest_ckpt)\n",
    "\n",
    "            # Data dir mirrors actual subset name\n",
    "            data_dir = os.path.join(data_path, base_model_nm, length_nm, subset)\n",
    "\n",
    "            # Result dir mirrors lr-dir\n",
    "            res_pdir = f\"{base_path}/RESULT/{lr_dir}/{base_model_nm}_{length_nm}_{subset}\"\n",
    "            os.makedirs(res_pdir, exist_ok=True)\n",
    "\n",
    "            jobs.append({\n",
    "                \"base_model_nm\": base_model_nm,\n",
    "                \"subset\": subset,\n",
    "                \"lr_dir\": lr_dir,\n",
    "                \"model_path\": model_path,\n",
    "                \"data_dir\": data_dir,\n",
    "                \"res_pdir\": res_pdir,\n",
    "            })\n",
    "\n",
    "print(f\"-> Collected {len(jobs)} jobs\")\n",
    "for j in jobs:\n",
    "    print(f\"** {j['base_model_nm']} | {j['subset']} | {j['lr_dir']} | {os.path.basename(j['model_path'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e4bbd24-6e5c-4dbd-882a-2251e26191f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Saved 2 jobs to /data/private/psurana/TSProm/src/files/jobs1.json\n"
     ]
    }
   ],
   "source": [
    "# write for spleen and muscle jobs.json\n",
    "out_path = Path(\"/data/private/psurana/TSProm/src/files/jobs1.json\")\n",
    "\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(jobs, f, indent=4)\n",
    "\n",
    "print(f\"[ok] Saved {len(jobs)} jobs to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa518c6f-4de8-4148-8c83-74d30e2f1f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model_nm': 'TSp_vs_genNullseqs',\n",
       " 'subset': 'tspmuscle_genNullseqs',\n",
       " 'lr_dir': 'lr3e-5_ep10',\n",
       " 'model_path': '/data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/july_2025_mmseq/TSp_vs_genNullseqs/3k/tspmuscle_genNullseqs/lr3e-5_ep10/checkpoint-300',\n",
       " 'data_dir': '/data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/data/jul_2025/split/TSp_vs_genNullseqs/3k/tspmuscle_genNullseqs',\n",
       " 'res_pdir': '/data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/july_2025_mmseq/RESULT/lr3e-5_ep10/TSp_vs_genNullseqs_3k_tspmuscle_genNullseqs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70b1ac84-752e-4a84-947d-272b641b6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_pred_file(split, folder):\n",
    "    \"\"\"Prefer *_all_predictions.csv, else *_correct.csv, else first {split}*.csv.\"\"\"\n",
    "    preferred = [\n",
    "        os.path.join(folder, f\"{split}_all_predictions.csv\"),\n",
    "        os.path.join(folder, f\"{split}_correct.csv\"),\n",
    "    ]\n",
    "    for p in preferred:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    matches = sorted(glob.glob(os.path.join(folder, f\"{split}*.csv\")))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def safe_colbind(df_left, df_right):\n",
    "    \"\"\"Column-bind with sensible handling of duplicate sequence columns.\"\"\"\n",
    "    left = df_left.copy()\n",
    "    right = df_right.copy()\n",
    "\n",
    "    # Normalize headers\n",
    "    left.columns  = [c.strip() for c in left.columns]\n",
    "    right.columns = [c.strip() for c in right.columns]\n",
    "\n",
    "    if \"sequence\" in left.columns and \"sequence\" in right.columns:\n",
    "        if len(left) == len(right) and left[\"sequence\"].astype(str).equals(right[\"sequence\"].astype(str)):\n",
    "            right = right.drop(columns=[\"sequence\"], errors=\"ignore\")\n",
    "            return pd.concat([left, right], axis=1)\n",
    "        left[\"_order_idx\"] = range(len(left))\n",
    "        merged = pd.merge(left, right, on=\"sequence\", how=\"left\", suffixes=(\"\", \"_pred\"))\n",
    "        merged = merged.sort_values(\"_order_idx\").drop(columns=[\"_order_idx\"])\n",
    "        return merged\n",
    "\n",
    "    return pd.concat([left, right], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c385cc4-f4a4-4dea-93d3-d3c2cfbe080d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined CSV written for tspmuscle_nonPromHu ‚Üí /data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/july_2025_mmseq/RESULT/lr3e-5_ep10/TSp_vs_nonProm_3k_tspmuscle_nonPromHu/preds/2_combined.csv\n",
      "‚úÖ Combined CSV written for tspmuscle_genNullseqs ‚Üí /data/projects/dna/pallavi/DNABERT_runs/DATA_RUN/dnabert2_FineTune_Zhihan_attention_extracted/july_2025_mmseq/RESULT/lr3e-5_ep10/TSp_vs_genNullseqs_3k_tspmuscle_genNullseqs/preds/2_combined.csv\n"
     ]
    }
   ],
   "source": [
    "for job in jobs:\n",
    "    preds_dir = os.path.join(job[\"res_pdir\"], \"preds\")\n",
    "    data_dir  = job[\"data_dir\"]\n",
    "    os.makedirs(preds_dir, exist_ok=True)\n",
    "\n",
    "    all_splits = []\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        data_csv = os.path.join(data_dir, f\"{split}.csv\")\n",
    "        if not os.path.exists(data_csv):\n",
    "            print(f\"‚ö†Ô∏è Missing data CSV for {split} in {data_dir}. Skipping.\")\n",
    "            continue\n",
    "        df_data = pd.read_csv(data_csv)\n",
    "\n",
    "        pred_csv = pick_pred_file(split, preds_dir)\n",
    "        if pred_csv is None:\n",
    "            print(f\"‚ö†Ô∏è No prediction file found for {split} in {preds_dir}. Skipping.\")\n",
    "            continue\n",
    "        df_pred = pd.read_csv(pred_csv)\n",
    "\n",
    "        combined = safe_colbind(df_data, df_pred)\n",
    "        combined.insert(0, \"split\", split)\n",
    "        all_splits.append(combined)\n",
    "\n",
    "    if not all_splits:\n",
    "        print(f\"‚ùå No splits combined for {job['subset']} ({job['res_pdir']})\")\n",
    "        continue\n",
    "\n",
    "    final_df = pd.concat(all_splits, ignore_index=True)\n",
    "    \n",
    "    # üîó merge with *_balanced.csv if exists\n",
    "    bal_files = glob.glob(os.path.join(data_dir, \"*_balanced.csv\"))\n",
    "    if bal_files:\n",
    "        df_bal = pd.read_csv(bal_files[0])\n",
    "        final_df = pd.merge(final_df, df_bal, on=[\"Sequence\", \"Label\"], how=\"left\")\n",
    "        \n",
    "    out_path = os.path.join(preds_dir, \"2_combined.csv\")\n",
    "    final_df.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ Combined CSV written for {job['subset']} ‚Üí {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005d54e-2111-4771-b971-5751c75bee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
